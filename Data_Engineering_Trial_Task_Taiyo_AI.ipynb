{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOw7Rv9sRPz8l26aVleEkVa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukulgaikwad/NY-City-ETL-Pipeline/blob/main/Data_Engineering_Trial_Task_Taiyo_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyvNsLoyInkq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Engineering Trial Task\n",
        "Objective:\n",
        "Find, scrape, standardize, and continuously update data regarding construction and infrastructure projects and tenders in the state of California"
      ],
      "metadata": {
        "id": "zpfCRPJVIyU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Research and Data Sourcing**\n",
        "art 1: Researching and Sourcing Data\n",
        "Objective: List 5-10 credible data sources dealing with construction and infrastructure projects and tenders in California.\n",
        "\n",
        "Approach:\n",
        "\n",
        "Researching\n",
        "\n",
        "The use of search engines to find official government websites dealing with infrastructure, construction, and tender projects in California.\n",
        "Look out for city or state-level project databases, procurement portals, and public works departments.\n",
        "Try such keywords as \"California construction projects\", \"infrastructure tenders California\", and \"California public works projects.\"\n",
        "\n",
        "Example data sources:\n",
        "\n",
        "Richmond City Major Projects: Richmond Projects\n",
        "\n",
        "City of Eureka Current Projects: Eureka Current Projects\n",
        "\n",
        "City of Eureka Completed Projects: Eureka Completed Projects\n",
        "\n",
        "Cal-eProcure Portal: Cal-eProcure\n",
        "\n",
        "City of Irvine Major Projects: Irvine Major Projects\n",
        "\n",
        "Additional Sources: California Department of Transportation, California Infrastructure and Economic Development Bank.\n"
      ],
      "metadata": {
        "id": "4t4_Ug-DJcGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ceQ_4D6xJf6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Data Extraction and Standardization**\n",
        "\n",
        "Objective: Scrape the identified data sources and standardize the extracted data according to the given schema.\n",
        "\n",
        "**Tools and Libraries:**\n",
        "\n",
        "Python Libraries:\n",
        "BeautifulSoup, parsing HTML.\n",
        "\n",
        "Selenium, handling dynamic content.\n",
        "requests, for REST API interactions.\n",
        "\n",
        "Pandas, for data manipulation and standardization.\n",
        "\n",
        "uuid, generation of unique ids (aug_id).\n",
        "\n",
        "Machine Learning Models:\n",
        "OpenAI API or other language models in transforming unstructured text into structured data.\n",
        "\n",
        "Data Extraction Process:\n",
        "Static Web Pages:\n",
        "\n",
        "\n",
        "Extract HTML elements containing project data, like titles, descriptions, or budgets, using BeautifulSoup. Parse the necessary attributes and keep them in structured form, for example, in CSV or a DataFrame. Dynamic Web Pages:\n",
        "\n",
        "\n",
        "Use Selenium to automate browser actions and capture dynamically loaded content.\n",
        "Extract the data similarly as in static pages after the content is loaded. REST APIs:\n",
        "\n",
        "\n",
        "Make contact with available APIs using requests.\n",
        "Parse JSON or XML responses and map fields to the desired schema.\n",
        "Dealing with Unstructured Data:\n",
        "\n",
        "\n",
        "Language models such as OpenAI GPT or Mistral 7B can be used for unstructured text, like project descriptions, in order to turn that into structured attributes like budget and timestamp.\n",
        "\n",
        "Data Standardization:\n",
        "\n",
        "Schema Mapping:\n",
        "\n",
        "Map the extracted data to the schema mentioned in Table 2.\n",
        "\n",
        "original_id: Unique identifier from source\n",
        "\n",
        "aug_id: Generate a unique identifier with uuid.uuid4()\n",
        "\n",
        "country_name: \"United States\"\n",
        "\n",
        "country_code: \"USA\".\n",
        "\n",
        "region_name and region_code: World Bank classifi-cations are to be used.\n",
        "\n",
        " latitude/longitude: Extracted from project data, if available.\n",
        "\n",
        "  timestamp and timestamp_label: Parse dates and label them appropri-ately such as published_date. Data Transformation:\n",
        "\n",
        "The data will be transformed and cleaned by using Pandas. There will be consistency across different data-sets, particularly for categorical variables such as sector, subsector. Sample Data Preparation:\n",
        "\n",
        "A sample dataset is created by scraping a few entries from each source identified earlier and standardizing them. Continuous Data Updating: Automation with Cron Jobs:\n",
        "\n",
        "Schedule regular data scraping using cron jobs for updating continuously.\n",
        "\n",
        " Add logging to track updates and failures.\n",
        "\n",
        " Data Storage: The standardized data will be stored in a relational database such as MySQL or PostgreSQL for easy querying and management.\n",
        "\n",
        " The data schema should be under version control to track changes over time. Monitoring and Alerts: Monitoring scripts check the status of the scraping process.\n",
        "\n",
        " The system shall set off alerts in case of failure or discrepancy in the data. Deliverables: Python Scripts:\n",
        "\n",
        "Provide scrape, standardize, and update scripts. Add detailed comments and document each script. Sample Dataset:\n",
        "\n",
        "Provide a sample standardized dataset that will show how data from the different sources has been unified. Documentation:\n",
        "\n",
        "Document the scraping process, data standardization approach, and automation setup comprehensively. Include a README file that includes details on how to run the scripts and what each script does.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Let's proceed step-by-step with the tasks mentioned in the solution. The first step involves researching and identifying reliable data sources, followed by scripting the extraction and standardization process.**\n",
        "\n",
        "Step 1: Research and Data Sourcing\n",
        "We'll start by confirming and documenting 5-10 reliable data sources for construction and infrastructure projects in California.\n",
        "\n",
        "**Confirmed Data Sources:**\n",
        "\n",
        "City of Richmond - Major Projects:\n",
        "\n",
        "URL: https://www.ci.richmond.ca.us/1404/Major-Projects\n",
        "\n",
        "Description: Provides details on ongoing and planned infrastructure projects within Richmond, CA.\n",
        "\n",
        "City of Eureka - Current Projects:\n",
        "\n",
        "URL: https://www.eurekaca.gov/744/Current-Projects\n",
        "\n",
        "Description: Lists current infrastructure projects, providing project descriptions, statuses, and budget information.\n",
        "\n",
        "City of Eureka - Completed Projects:\n",
        "\n",
        "URL: https://www.eurekaca.gov/305/Completed-Projects\n",
        "\n",
        "\n",
        "Description: Information on projects that have been completed, with details on costs and completion timelines.\n",
        "\n",
        "Cal-eProcure (California Statewide Procurement Portal):\n",
        "\n",
        "URL: https://caleprocure.ca.gov/pages/Events-BS3/event-search.aspx\n",
        "\n",
        "Description: A comprehensive portal for statewide tenders, contracts, and procurement events related to public infrastructure.\n",
        "\n",
        "City of Irvine - Major Projects:\n",
        "\n",
        "URL: Irvine Major Projects\n",
        "\n",
        "Description: Interactive map with details on major infrastructure projects in the city.\n",
        "\n",
        "California Department of Transportation (Caltrans):\n",
        "\n",
        "URL: https://dot.ca.gov/\n",
        "\n",
        "Description: Comprehensive information on transportation infrastructure projects across California.\n",
        "\n",
        "Los Angeles County Public Works:\n",
        "\n",
        "URL: LA County Public Works\n",
        "\n",
        "Description: Information on public works projects, including roadways, water systems, and other infrastructure within Los Angeles County.\n"
      ],
      "metadata": {
        "id": "dcwIlHcAKP-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Data Extraction and Standardization**\n",
        "\n",
        "Objective: Write Python scripts to extract and standardize data from the identified sources.\n",
        "\n",
        "Sub-step 2.\n",
        "\n",
        "1: Setting Up the Environment\n",
        "Install Necessary Python Libraries:\n",
        "\n",
        "**Use the following command to install the required libraries**\n",
        "\n",
        "pip install requests beautifulsoup4 selenium pandas lxml uuid\n",
        "\n",
        "2.Setup ChromeDriver for Selenium:\n",
        "\n",
        "**Download and configure chromedriver if needed, ensuring that it is compatible with the installed version of Chrome.**\n",
        "\n",
        "Sub-step 2.2: Data Extraction\n",
        "\n",
        "Example: Extracting Data from Richmond Major Projects\n",
        "\n",
        "We'll start by extracting data from the Richmond Major Projects page.\n",
        "\n",
        "**Script for Static Page Scraping (Richmond Projects):**"
      ],
      "metadata": {
        "id": "Hb6s7bO1NZn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Define the URL\n",
        "url = \"http://www.ci.richmond.ca.us/1404/Major-Projects\"\n",
        "\n",
        "# Send a GET request to the page\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract project data\n",
        "projects = []\n",
        "for item in soup.find_all('div', class_='project-container'):\n",
        "    project = {}\n",
        "    project['original_id'] = item.get('id', 'N/A')\n",
        "    project['aug_id'] = str(uuid.uuid4())\n",
        "    project['title'] = item.find('h3').text.strip()\n",
        "    project['description'] = item.find('p').text.strip()\n",
        "    # Additional fields to be extracted and mapped based on the content\n",
        "    projects.append(project)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(projects)\n",
        "\n",
        "# Display or save the DataFrame\n",
        "print(df.head())\n",
        "df.to_csv('richmond_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "uXRfbpITOJfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Script for Dynamic Content (Using Selenium for Eureka Projects):**"
      ],
      "metadata": {
        "id": "pT4llDwgONb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Initialize the Selenium WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Open the Eureka Projects page\n",
        "url = \"http://www.eurekaca.gov/744/Current-Projects\"\n",
        "driver.get(url)\n",
        "\n",
        "# Extract project elements\n",
        "projects = []\n",
        "project_elements = driver.find_elements(By.CLASS_NAME, 'project-item')  # Update based on the actual class used\n",
        "\n",
        "for element in project_elements:\n",
        "    project = {}\n",
        "    project['original_id'] = element.get_attribute('id')\n",
        "    project['aug_id'] = str(uuid.uuid4())\n",
        "    project['title'] = element.find_element(By.TAG_NAME, 'h3').text\n",
        "    project['description'] = element.find_element(By.TAG_NAME, 'p').text\n",
        "    # Additional fields\n",
        "    projects.append(project)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(projects)\n",
        "df.to_csv('eureka_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "UAVn7B3sOPiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sub-step 2.3: Standardizing the Data**\n",
        "\n",
        "**Standardization Script:**"
      ],
      "metadata": {
        "id": "6zoN8qUiOThB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame (from one of the scrapers)\n",
        "df = pd.read_csv('richmond_projects.csv')\n",
        "\n",
        "# Standardization based on Table 2 schema\n",
        "df_standardized = pd.DataFrame({\n",
        "    'original_id': df['original_id'],\n",
        "    'aug_id': df['aug_id'],\n",
        "    'country_name': 'United States',\n",
        "    'country_code': 'USA',\n",
        "    'region_name': 'California',\n",
        "    'region_code': 'NA',\n",
        "    'latitude': None,  # To be extracted or set as None\n",
        "    'longitude': None,  # To be extracted or set as None\n",
        "    'url': url,\n",
        "    'title': df['title'],\n",
        "    'description': df['description'],\n",
        "    'status': 'N/A',  # To be updated based on actual data\n",
        "    'timestamp': 'N/A',  # To be extracted\n",
        "    'timestamp_label': 'published_date',  # Or another appropriate label\n",
        "    'budget': None,  # To be extracted if available\n",
        "    'budget_label': 'N/A',\n",
        "    'currency': 'USD',\n",
        "    'sector': 'Infrastructure',  # Example sector\n",
        "    'subsector': 'Public Works',  # Example subsector\n",
        "    'document_urls': 'N/A'  # To be updated\n",
        "})\n",
        "\n",
        "# Save the standardized DataFrame\n",
        "df_standardized.to_csv('standardized_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "RenACCDvOcLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3: Continuous Data Updating**\n",
        "\n",
        "\n",
        "Objective: Implement automation for continuous updates.\n",
        "\n",
        "Setting up Cron Jobs:\n",
        "\n",
        "Schedule the scraping scripts to run at regular intervals (e.g., daily or weekly) using cron jobs on a Unix-based system.\n",
        "Monitoring and Logging:\n",
        "\n",
        "Implement logging in the Python scripts to track execution and errors.\n",
        "Use a monitoring tool like cronitor to track the health of cron jobs.\n",
        "Storing Data in a Database:\n",
        "\n",
        "Store the standardized data in a relational database.\n",
        "Design a schema in MySQL or PostgreSQL matching the data structure.\n",
        "Use SQLAlchemy in Python to automate database updates."
      ],
      "metadata": {
        "id": "BjPQrwIZOhQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Deliverables:**\n",
        "\n",
        "Python Scripts:\n",
        "\n",
        "Scripts for each data source.\n",
        "Scripts for standardization and continuous updates.\n",
        "\n",
        "Sample Data:\n",
        "\n",
        "Provide sample data in a standardized format (CSV or JSON).\n",
        "\n",
        "Documentation:\n",
        "\n",
        "Write documentation for each script.\n"
      ],
      "metadata": {
        "id": "3RcrKh7zOqQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Script 1: Static Page Scraping (Richmond Major Projects)**"
      ],
      "metadata": {
        "id": "7T3OCro0O7FR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Define the URL\n",
        "url = \"http://www.ci.richmond.ca.us/1404/Major-Projects\"\n",
        "\n",
        "# Send a GET request to the page\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# Extract project data\n",
        "projects = []\n",
        "for item in soup.find_all('div', class_='project-container'):\n",
        "    project = {}\n",
        "    project['original_id'] = item.get('id', 'N/A')\n",
        "    project['aug_id'] = str(uuid.uuid4())\n",
        "    project['title'] = item.find('h3').text.strip()\n",
        "    project['description'] = item.find('p').text.strip()\n",
        "    # Additional fields to be extracted and mapped based on the content\n",
        "    projects.append(project)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(projects)\n",
        "\n",
        "# Display or save the DataFrame\n",
        "print(df.head())\n",
        "df.to_csv('richmond_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "-EO9IznqO-FN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Notes: Script 1\n",
        "\n",
        "Functionality: The script sends a request to the Richmond Major Projects webpage and extracts project titles and descriptions.\n",
        "Scalability: The script can be adapted to handle multiple pages or additional data fields.\n",
        "Next Steps: The script should be expanded to handle more fields like status, budget, latitude, longitude, and timestamp."
      ],
      "metadata": {
        "id": "dd54N5ljPJCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Script 2: Dynamic Content Scraping (Selenium for Eureka Projects)**"
      ],
      "metadata": {
        "id": "XEjdlWJMPMdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import pandas as pd\n",
        "import uuid\n",
        "\n",
        "# Initialize the Selenium WebDriver\n",
        "driver = webdriver.Chrome()\n",
        "\n",
        "# Open the Eureka Projects page\n",
        "url = \"http://www.eurekaca.gov/744/Current-Projects\"\n",
        "driver.get(url)\n",
        "\n",
        "# Extract project elements\n",
        "projects = []\n",
        "project_elements = driver.find_elements(By.CLASS_NAME, 'project-item')  # Update based on the actual class used\n",
        "\n",
        "for element in project_elements:\n",
        "    project = {}\n",
        "    project['original_id'] = element.get_attribute('id')\n",
        "    project['aug_id'] = str(uuid.uuid4())\n",
        "    project['title'] = element.find_element(By.TAG_NAME, 'h3').text\n",
        "    project['description'] = element.find_element(By.TAG_NAME, 'p').text\n",
        "    # Additional fields\n",
        "    projects.append(project)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(projects)\n",
        "df.to_csv('eureka_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "daPqWnRePOOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Notes:Script 2\n",
        "\n",
        "Functionality: The script uses Selenium to interact with a dynamically loaded webpage, extracting project titles and descriptions.\n",
        "Scalability: Like the previous script, this one can be expanded to extract additional fields and handle multiple pages.\n",
        "Next Steps: Implement error handling (e.g., try-except blocks) for robustness, especially for elements that might not load properly."
      ],
      "metadata": {
        "id": "q9I_6zkfPRw5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Script 3: Data Standardization**"
      ],
      "metadata": {
        "id": "L19NDSUAPW3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example DataFrame (from one of the scrapers)\n",
        "df = pd.read_csv('richmond_projects.csv')\n",
        "\n",
        "# Standardization based on Table 2 schema\n",
        "df_standardized = pd.DataFrame({\n",
        "    'original_id': df['original_id'],\n",
        "    'aug_id': df['aug_id'],\n",
        "    'country_name': 'United States',\n",
        "    'country_code': 'USA',\n",
        "    'region_name': 'California',\n",
        "    'region_code': 'NA',\n",
        "    'latitude': None,  # To be extracted or set as None\n",
        "    'longitude': None,  # To be extracted or set as None\n",
        "    'url': 'http://www.ci.richmond.ca.us/1404/Major-Projects',\n",
        "    'title': df['title'],\n",
        "    'description': df['description'],\n",
        "    'status': 'N/A',  # To be updated based on actual data\n",
        "    'timestamp': 'N/A',  # To be extracted\n",
        "    'timestamp_label': 'published_date',  # Or another appropriate label\n",
        "    'budget': None,  # To be extracted if available\n",
        "    'budget_label': 'N/A',\n",
        "    'currency': 'USD',\n",
        "    'sector': 'Infrastructure',  # Example sector\n",
        "    'subsector': 'Public Works',  # Example subsector\n",
        "    'document_urls': 'N/A'  # To be updated\n",
        "})\n",
        "\n",
        "# Save the standardized DataFrame\n",
        "df_standardized.to_csv('standardized_projects.csv', index=False)\n"
      ],
      "metadata": {
        "id": "6ortE6BHPY7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review Notes:Script 3\n",
        "\n",
        "Functionality: The script standardizes the scraped data according to the schema provided in the PDF.\n",
        "Scalability: It can be applied to datasets from other sources as well. The placeholder values (None, 'N/A') should be populated with actual data where possible.\n",
        "Next Steps: Enhance the script to dynamically populate fields like latitude, longitude, status, timestamp, and budget by extracting from the source or inferring where possible."
      ],
      "metadata": {
        "id": "UkktI4rcPcGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next Steps: Automation and Database Setup**"
      ],
      "metadata": {
        "id": "Oe4eaEh7PjKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sub-step 3.\n",
        "\n",
        "1: Automation with Cron Jobs**\n",
        "\n",
        "Setting Up Cron Jobs:\n",
        "\n",
        "On a Unix-based system (Linux or macOS), use the following steps:\n",
        "\n",
        "**crontab -e**\n",
        "\n",
        "Add a cron job to run the scraping script daily at 2 AM:\n",
        "\n",
        "0 2 * * * /usr/bin/python3 /path/to/richmond_scrape.py\n",
        "\n",
        "0 2 * * * /usr/bin/python3 /path/to/eureka_scrape.py\n",
        "\n",
        "\n",
        "2.Logging and Error Handling:\n",
        "\n",
        "Modify the scripts to include logging\n",
        "\n",
        "import logging\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WV-eNxl7PqJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(filename='scrape.log', level=logging.INFO)\n",
        "\n",
        "logging.info('Starting scraping job')"
      ],
      "metadata": {
        "id": "OHmpWHZrQKb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Monitor Cron Jobs:\n",
        "\n",
        "Use tools like Cronitor or custom scripts to monitor the health of your cron jobs."
      ],
      "metadata": {
        "id": "MdsRfYtYQNg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 3.2: Database Setup\n",
        "\n",
        "\n",
        "Database Design:\n",
        "\n",
        "Use MySQL or PostgreSQL for storing the data.\n",
        "\n",
        "The table schema will reflect the standardized data structure"
      ],
      "metadata": {
        "id": "d2IKAL5oQWAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE projects (\n",
        "    original_id VARCHAR(255),\n",
        "    aug_id VARCHAR(36) PRIMARY KEY,\n",
        "    country_name VARCHAR(255),\n",
        "    country_code VARCHAR(3),\n",
        "    region_name VARCHAR(255),\n",
        "    region_code VARCHAR(3),\n",
        "    latitude FLOAT,\n",
        "    longitude FLOAT,\n",
        "    url TEXT,\n",
        "    title TEXT,\n",
        "    description TEXT,\n",
        "    status VARCHAR(50),\n",
        "    timestamp DATE,\n",
        "    timestamp_label VARCHAR(50),\n",
        "    budget BIGINT,\n",
        "    budget_label VARCHAR(255),\n",
        "    currency VARCHAR(3),\n",
        "    sector VARCHAR(255),\n",
        "    subsector VARCHAR(255),\n",
        "    document_urls TEXT\n",
        ");\n"
      ],
      "metadata": {
        "id": "ftQcuAVwQbrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Insertion Script:\n",
        "\n",
        "Modify the standardization script to insert data into the database:"
      ],
      "metadata": {
        "id": "fybxVvqZQgxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlalchemy\n",
        "\n",
        "# Database connection\n",
        "engine = sqlalchemy.create_engine('mysql+pymysql://username:password@localhost:3306/database_name')\n",
        "\n",
        "# Insert DataFrame into the database\n",
        "df_standardized.to_sql('projects', con=engine, if_exists='append', index=False)\n"
      ],
      "metadata": {
        "id": "ZS5hGI5mQjfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Management:\n",
        "\n",
        "Implement mechanisms to handle duplicate data entries and update existing records if necessary."
      ],
      "metadata": {
        "id": "oNioacjcQmqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Steps:**\n",
        "\n",
        "Execute the automation and ensure data is scraped and stored in the database regularly.\n",
        "\n",
        "Verify the database for consistency and accuracy of the stored data.\n",
        "\n",
        "Set up alerts and monitoring for any issues with scraping or data insertion."
      ],
      "metadata": {
        "id": "dWSW8_JsQrYe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's proceed with the automation and database setup.**\n",
        "\n",
        "Automation with Cron Jobs\n",
        "\n",
        "Objective:\n",
        " Automate the execution of the Python scripts for scraping data and standardizing it.\n",
        "\n",
        " Step 1: Configure Cron Jobs\n",
        "\n",
        "Access the Cron Tab:\n",
        "\n",
        "Open your terminal and access the cron jobs configuration"
      ],
      "metadata": {
        "id": "Jmtr3-y6Q0lp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crontab -e\n"
      ],
      "metadata": {
        "id": "yUnNN5KdREVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up Cron Jobs:\n",
        "\n",
        "Add the following lines to schedule the scraping scripts to run daily at 2 AM:"
      ],
      "metadata": {
        "id": "466aT5TzRGWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Schedule the Richmond scraping script to run daily at 2 AM\n",
        "0 2 * * * /usr/bin/python3 /path/to/richmond_scrape.py >> /path/to/scrape.log 2>&1\n",
        "\n",
        "# Schedule the Eureka scraping script to run daily at 2 AM\n",
        "0 2 * * * /usr/bin/python3 /path/to/eureka_scrape.py >> /path/to/scrape.log 2>&1\n"
      ],
      "metadata": {
        "id": "HSD1K7YrRJj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace /path/to/ with the actual path where your Python scripts are located."
      ],
      "metadata": {
        "id": "Dvg1UUTBRNUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify Cron Jobs:\n",
        "\n",
        "To ensure the cron jobs are set up correctly, list all active cron jobs\n",
        "\n"
      ],
      "metadata": {
        "id": "X0Mkfr-MRP5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crontab -l\n"
      ],
      "metadata": {
        "id": "2jb3GAubRT6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2: Logging and Error Handling**\n",
        "\n",
        "Modify Scripts for Logging:\n",
        "\n",
        "Add logging functionality to capture the script's execution details"
      ],
      "metadata": {
        "id": "7vW7ftOKRXOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(filename='/path/to/scrape.log', level=logging.INFO,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "logging.info('Starting Richmond scraping job')\n"
      ],
      "metadata": {
        "id": "oFylEwC-Recq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error Handling:\n",
        "\n",
        "Implement 'try-except'\n",
        "\n",
        " blocks in your scripts to handle potential errors and log them"
      ],
      "metadata": {
        "id": "65dFlegLRhi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Your scraping logic\n",
        "    logging.info('Scraping completed successfully')\n",
        "except Exception as e:\n",
        "    logging.error(f'Error during scraping: {e}')\n"
      ],
      "metadata": {
        "id": "MpVcI_y4Ronj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Database Setup**\n",
        "\n",
        "Objective: Store the scraped and standardized data in a relational database for easy access and querying.\n",
        "\n",
        "Step 1: Set Up the Database\n",
        "Choose a Database:\n",
        "\n",
        "MySQL or PostgreSQL are both good choices. For this example, let's use MySQL.\n",
        "Install MySQL:\n",
        "\n",
        "If you don't have MySQL installed, you can install it using the following command:\n",
        "\n",
        "if you dont have\n",
        "\n",
        "\n",
        "sudo apt-get install mysql-server\n",
        "\n",
        "Create a Database and Table:\n",
        "\n",
        "Log into MySQL:\n"
      ],
      "metadata": {
        "id": "tEDYtHp9RvHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mysql -u root -p\n"
      ],
      "metadata": {
        "id": "7EQMCpWESHBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new database:"
      ],
      "metadata": {
        "id": "4KDuYJAmSI7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE DATABASE project_data;\n",
        "USE project_data;\n"
      ],
      "metadata": {
        "id": "ayS6UlAJSJqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the projects table:"
      ],
      "metadata": {
        "id": "qndvXwykSMp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CREATE TABLE projects (\n",
        "    original_id VARCHAR(255),\n",
        "    aug_id VARCHAR(36) PRIMARY KEY,\n",
        "    country_name VARCHAR(255),\n",
        "    country_code VARCHAR(3),\n",
        "    region_name VARCHAR(255),\n",
        "    region_code VARCHAR(3),\n",
        "    latitude FLOAT,\n",
        "    longitude FLOAT,\n",
        "    url TEXT,\n",
        "    title TEXT,\n",
        "    description TEXT,\n",
        "    status VARCHAR(50),\n",
        "    timestamp DATE,\n",
        "    timestamp_label VARCHAR(50),\n",
        "    budget BIGINT,\n",
        "    budget_label VARCHAR(255),\n",
        "    currency VARCHAR(3),\n",
        "    sector VARCHAR(255),\n",
        "    subsector VARCHAR(255),\n",
        "    document_urls TEXT\n",
        ");\n"
      ],
      "metadata": {
        "id": "d8ikpGv0SNZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Insert Data into the Database\n",
        "\n",
        "\n",
        "Modify the Standardization Script for Database Insertion:\n",
        "\n",
        "After standardizing the data, insert it into the MySQL database"
      ],
      "metadata": {
        "id": "_GNGW09pSRvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlalchemy\n",
        "\n",
        "# Database connection string\n",
        "engine = sqlalchemy.create_engine('mysql+pymysql://username:password@localhost:3306/project_data')\n",
        "\n",
        "# Load the standardized DataFrame\n",
        "df_standardized = pd.read_csv('/path/to/standardized_projects.csv')\n",
        "\n",
        "# Insert data into the database\n",
        "df_standardized.to_sql('projects', con=engine, if_exists='append', index=False)\n"
      ],
      "metadata": {
        "id": "1LPEs-IOSUzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Prevent Duplicate Entries:\n",
        "\n",
        "Ensure the aug_id is set as the primary key to prevent duplicates. You can modify the script to check for existing records before insertion.\n",
        "\n",
        "3.Schedule Data Insertion:\n",
        "\n",
        "Integrate this data insertion logic into your cron job scripts, ensuring that after data is scraped and standardized, it is automatically inserted into the database."
      ],
      "metadata": {
        "id": "IDnKXyP0SaoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitoring and Alerts\n",
        "Objective: Monitor the health of cron jobs and set up alerts for any failures.\n",
        "\n",
        "Use cronitor.io:\n",
        "\n",
        "Register for cronitor.io or similar service to monitor your cron jobs. This service will notify you if a job fails or doesn't run as expected.\n",
        "Set Up Alerts:\n",
        "\n",
        "Configure alerts in cronitor.io to send notifications via email or SMS if there are issues with the cron jobs.\n",
        "Final Verification and Execution\n",
        "Test the Setup:\n",
        "\n",
        "Manually run the Python scripts to verify that data is scraped, standardized, and inserted into the database without errors.\n",
        "Check Database Entries:\n",
        "\n",
        "Query the database to ensure the data is being stored correctly"
      ],
      "metadata": {
        "id": "wPI_KRLTSmBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SELECT * FROM projects LIMIT 10;\n"
      ],
      "metadata": {
        "id": "ajh1IeXFStAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monitor Logs:\n",
        "\n",
        "Regularly check the logs generated by the cron jobs to ensure there are no errors and that the jobs are running as expected."
      ],
      "metadata": {
        "id": "mwG_YpsmSvUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion and Next Steps**\n",
        "\n",
        "Next Steps: After confirming that the automation and database setup is functioning properly, you may consider further enhancements such as:\n",
        "Developing a dashboard for visualizing the scraped data.\n",
        "Implementing data cleaning processes to improve data quality.\n",
        "\n",
        "Creating a chatbot for querying the database using natural language."
      ],
      "metadata": {
        "id": "sMYVPwGLSyyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Dashboard for Visualizing Scraped Data**"
      ],
      "metadata": {
        "id": "HmIV5UI7S4Mh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective: Develop a dashboard that visualizes the data stored in the database, providing insights into various construction and infrastructure projects.\n",
        "\n",
        "Sub-step 4.1: Set Up a Visualization Tool\n",
        "Choose a Visualization Framework:\n",
        "\n",
        "Options: You can use tools like Tableau, Power BI, or open-source solutions like Plotly Dash or Grafana.\n",
        "For this example, we'll use Plotly Dash, which integrates seamlessly with Python and provides interactive, web-based visualizations.\n",
        "Install Dash:\n",
        "\n",
        "Install Dash and Plotly"
      ],
      "metadata": {
        "id": "IKimLqzwS78W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dash dash-bootstrap-components plotly\n"
      ],
      "metadata": {
        "id": "nRHvUPXLS-MP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 4.2: Create a Basic Dashboard\n",
        "Connect Dashboard to the Database:\n",
        "\n",
        "Fetch data from the MySQL database for visualization"
      ],
      "metadata": {
        "id": "ciQAmwUwTEom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "import plotly.express as px\n",
        "import sqlalchemy\n",
        "import pandas as pd\n",
        "\n",
        "# Database connection\n",
        "engine = sqlalchemy.create_engine('mysql+pymysql://username:password@localhost:3306/project_data')\n",
        "\n",
        "# Query data from the database\n",
        "query = \"SELECT * FROM projects\"\n",
        "df = pd.read_sql(query, con=engine)\n",
        "\n",
        "# Initialize the Dash app\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "# Example plot: Number of projects by sector\n",
        "fig = px.bar(df, x='sector', title='Number of Projects by Sector')\n",
        "\n",
        "# Define the layout of the dashboard\n",
        "app.layout = html.Div(children=[\n",
        "    html.H1(children='California Construction Projects Dashboard'),\n",
        "    dcc.Graph(id='example-graph', figure=fig)\n",
        "])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n"
      ],
      "metadata": {
        "id": "ictte0iLTHbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the Dashboard:"
      ],
      "metadata": {
        "id": "yX8frGmSTLNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python dash_app.py\n"
      ],
      "metadata": {
        "id": "ApLjcjEuTNDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 4.3: Enhance the Dashboard\n",
        "Add More Visualizations:\n",
        "\n",
        "Include additional charts, such as:\n",
        "Map Visualization: Showing project locations based on latitude and longitude.\n",
        "Budget Analysis: Displaying projects based on their budget or funding status"
      ],
      "metadata": {
        "id": "scTNxE-RTPdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example map visualization\n",
        "fig_map = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", hover_name=\"title\",\n",
        "                            hover_data=[\"budget\"], color=\"sector\",\n",
        "                            zoom=6, height=300)\n",
        "fig_map.update_layout(mapbox_style=\"open-street-map\")\n",
        "fig_map.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "\n",
        "app.layout = html.Div(children=[\n",
        "    html.H1(children='California Construction Projects Dashboard'),\n",
        "    dcc.Graph(id='sector-graph', figure=fig),\n",
        "    dcc.Graph(id='map-graph', figure=fig_map)\n",
        "])\n"
      ],
      "metadata": {
        "id": "195BG4p3TRsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy the Dashboard:\n",
        "\n",
        "Deploy the dashboard on a web server for access by multiple users:\n",
        "Options: Deploy via Heroku, AWS, or DigitalOcean.\n",
        "Follow deployment guidelines for the chosen platform to set up a production-ready environment."
      ],
      "metadata": {
        "id": "Kvz-qhnNTUxo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Implement a Chatbot for Database Querying**\n",
        "\n",
        "Objective: Create a chatbot that allows users to interact with the database using natural language queries.\n",
        "\n",
        "Sub-step 5.1: Set Up a Chatbot Framework\n",
        "Choose a Chatbot Framework:\n",
        "\n",
        "Options: Use frameworks like Rasa, Dialogflow, or build a simple bot with Flask and an LLM (like OpenAI GPT).\n",
        "Install Necessary Libraries:\n",
        "\n",
        "For a basic Flask chatbot using OpenAI:"
      ],
      "metadata": {
        "id": "amHNtRZVTXU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install flask openai\n"
      ],
      "metadata": {
        "id": "chPgR-XcThMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 5.2: Create a Basic Flask Chatbot\n",
        "Basic Flask Setup:\n",
        "\n",
        "Create a simple Flask server"
      ],
      "metadata": {
        "id": "df3rQZLPTjzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import openai\n",
        "import sqlalchemy\n",
        "import pandas as pd\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Database connection\n",
        "engine = sqlalchemy.create_engine('mysql+pymysql://username:password@localhost:3306/project_data')\n",
        "\n",
        "# OpenAI API key (replace with your key)\n",
        "openai.api_key = 'your_openai_api_key'\n",
        "\n",
        "@app.route('/chat', methods=['POST'])\n",
        "def chat():\n",
        "    user_input = request.json['message']\n",
        "\n",
        "    # Process the query with OpenAI\n",
        "    response = openai.Completion.create(\n",
        "        engine=\"text-davinci-003\",\n",
        "        prompt=f\"Query the database: {user_input}\",\n",
        "        max_tokens=150\n",
        "    )\n",
        "\n",
        "    # Interpret the response and query the database\n",
        "    query = interpret_openai_response(response['choices'][0]['text'])\n",
        "    df = pd.read_sql(query, con=engine)\n",
        "\n",
        "    # Convert the result to JSON\n",
        "    result = df.to_dict(orient='records')\n",
        "    return jsonify(result)\n",
        "\n",
        "def interpret_openai_response(response):\n",
        "    # Logic to interpret OpenAI's response and translate it into an SQL query\n",
        "    # Example interpretation logic (simplified)\n",
        "    if \"budget\" in response:\n",
        "        return \"SELECT * FROM projects WHERE budget > 1000000\"\n",
        "    # More interpretation cases...\n",
        "    return \"SELECT * FROM projects\"\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "q3qgZ33pTmKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Chatbot:\n",
        "\n",
        "Run the Flask server"
      ],
      "metadata": {
        "id": "BwO9MwCqTpCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python chatbot.py\n"
      ],
      "metadata": {
        "id": "skMtQmesTqjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Send POST requests to the /chat endpoint with user queries, and the bot will return relevant data from the database."
      ],
      "metadata": {
        "id": "be0dvjNrTsu7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enhance the Chatbot:\n",
        "\n",
        "Improve the interpretation logic to handle a wide range of natural language queries.\n",
        "Implement user authentication and access control if needed."
      ],
      "metadata": {
        "id": "r-gwXfBJTu-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Data Cleaning and Quality Enhancement**\n",
        "\n",
        "Objective: Implement a process to clean and validate the data before it is stored in the database.\n",
        "\n",
        "Enhance the Data Standardization Script:\n",
        "\n",
        "Add checks for missing or inconsistent data"
      ],
      "metadata": {
        "id": "Vu9tc4GJTwBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Fill missing latitude/longitude with default values\n",
        "df_standardized['latitude'] = df_standardized['latitude'].fillna(0)\n",
        "df_standardized['longitude'] = df_standardized['longitude'].fillna(0)\n",
        "\n",
        "# Check for missing essential fields and handle them\n",
        "df_standardized.dropna(subset=['title', 'description'], inplace=True)\n"
      ],
      "metadata": {
        "id": "JP9gfWNUT2Ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Up Data Quality Alerts:\n",
        "\n",
        "Implement monitoring scripts that alert you if data quality issues are detected during the scraping process."
      ],
      "metadata": {
        "id": "3AfH_N0bT4Sa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Steps**\n",
        "\n",
        "Integrate All Components:\n",
        "\n",
        "Ensure that the scraping, database insertion, dashboard, and chatbot are all functioning together.\n",
        "Regularly update the database and validate the integrity of the data.\n",
        "Documentation and Deployment:\n",
        "\n",
        "Document the entire setup process, including how to maintain and update the system.\n",
        "Deploy the entire solution in a production environment for continuous operation."
      ],
      "metadata": {
        "id": "W6eN308cT6ua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's proceed by focusing on deploying the dashboard, setting up the chatbot, and then ensuring data quality enhancement. I'll guide you through each step in detail."
      ],
      "metadata": {
        "id": "lpdpX4OZUBrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4: Deploy the Dashboard**\n",
        "\n",
        "Objective: Deploy the Plotly Dash-based dashboard to a web server so it can be accessed by multiple users.\n",
        "\n",
        "Sub-step 4.1: Finalize the Dashboard\n",
        "Before deployment, let's finalize the dashboard with some essential visualizations.\n",
        "\n",
        "Finalize the Layout and Add Visualizations:"
      ],
      "metadata": {
        "id": "58R8GZyjUEaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "import dash_core_components as dcc\n",
        "import dash_html_components as html\n",
        "import plotly.express as px\n",
        "import sqlalchemy\n",
        "import pandas as pd\n",
        "\n",
        "# Database connection\n",
        "engine = sqlalchemy.create_engine('mysql+pymysql://username:password@localhost:3306/project_data')\n",
        "\n",
        "# Query data from the database\n",
        "query = \"SELECT * FROM projects\"\n",
        "df = pd.read_sql(query, con=engine)\n",
        "\n",
        "# Example bar plot: Number of projects by sector\n",
        "fig_sector = px.bar(df, x='sector', title='Number of Projects by Sector')\n",
        "\n",
        "# Example map visualization: Project locations\n",
        "fig_map = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\", hover_name=\"title\",\n",
        "                            hover_data=[\"budget\", \"status\"], color=\"sector\",\n",
        "                            zoom=5, height=300)\n",
        "fig_map.update_layout(mapbox_style=\"open-street-map\")\n",
        "fig_map.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
        "\n",
        "# Define the layout of the dashboard\n",
        "app = dash.Dash(__name__)\n",
        "\n",
        "app.layout = html.Div(children=[\n",
        "    html.H1(children='California Construction Projects Dashboard'),\n",
        "    dcc.Graph(id='sector-graph', figure=fig_sector),\n",
        "    dcc.Graph(id='map-graph', figure=fig_map)\n",
        "])\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run_server(debug=True)\n"
      ],
      "metadata": {
        "id": "bLudXU3hUjCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Dashboard Locally:\n",
        "\n",
        "Run the dashboard locally"
      ],
      "metadata": {
        "id": "l-My9mZYUno7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python dash_app.py\n"
      ],
      "metadata": {
        "id": "uoIyVJMHUoZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 4.2: Deploy to a Web Server\n",
        "Deploy Using Heroku:\n",
        "\n",
        "Install Heroku CLI:\n",
        "\n",
        "If you don't have Heroku CLI installed, you can download it from Heroku's website.\n",
        "Create a Procfile:\n",
        "\n",
        "Create a Procfile in the root directory of your project. This tells Heroku how to run your app:"
      ],
      "metadata": {
        "id": "aDt1mvZSUrwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "web: python dash_app.py\n"
      ],
      "metadata": {
        "id": "V7ZHnRS6Usjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Requirements File:\n",
        "\n",
        "Generate a requirements.txt to list all the dependencies:"
      ],
      "metadata": {
        "id": "ozo7XgOzUveq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze > requirements.txt\n"
      ],
      "metadata": {
        "id": "cgLdmuoWUw6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize a Git Repository:\n",
        "\n",
        "Initialize a git repository in your project directory:"
      ],
      "metadata": {
        "id": "FvSBwIJHUzib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git init\n",
        "git add .\n",
        "git commit -m \"Initial commit\"\n"
      ],
      "metadata": {
        "id": "5ew3DOTWU05D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy to Heroku:\n",
        "\n",
        "Create a new Heroku app and deploy:"
      ],
      "metadata": {
        "id": "qzv1sh6yU2sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heroku create your-app-name\n",
        "git push heroku master\n"
      ],
      "metadata": {
        "id": "DFuJtSfpU4WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the App:\n",
        "\n",
        "Once deployed, open the app"
      ],
      "metadata": {
        "id": "eehqsZqyU6W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "heroku open\n"
      ],
      "metadata": {
        "id": "Yydp3F4kU7tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy Using Other Platforms (e.g., AWS, DigitalOcean):\n",
        "\n",
        "For AWS, you can use Elastic Beanstalk or EC2 for deployment.\n",
        "For DigitalOcean, you can create a Droplet, install the necessary dependencies, and deploy the app using Nginx and Gunicorn."
      ],
      "metadata": {
        "id": "ygdkSVcwVAS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5: Implement a Chatbot for Database Querying**\n",
        "\n",
        "Objective: Set up a basic chatbot using Flask and OpenAI to interact with the database.\n",
        "\n",
        "Sub-step 5.1: Finalize the Flask Chatbot\n",
        "Improve the Interpretation Logic:\n",
        "\n",
        "Enhance the chatbot's logic to handle various types of queries."
      ],
      "metadata": {
        "id": "oOKQQy3eVDhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def interpret_openai_response(response):\n",
        "    # Simplified example to handle a few cases\n",
        "    if \"projects in\" in response:\n",
        "        city = response.split(\"projects in \")[1].split()[0]\n",
        "        return f\"SELECT * FROM projects WHERE region_name LIKE '%{city}%'\"\n",
        "    elif \"budget above\" in response:\n",
        "        amount = int(response.split(\"budget above \")[1].split()[0])\n",
        "        return f\"SELECT * FROM projects WHERE budget > {amount}\"\n",
        "    else:\n",
        "        return \"SELECT * FROM projects LIMIT 10\"\n"
      ],
      "metadata": {
        "id": "8tk946uYVHBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "est the Chatbot Locally:\n",
        "\n",
        "Run the Flask app and test querying:"
      ],
      "metadata": {
        "id": "ucApyC51VJC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python chatbot.py\n"
      ],
      "metadata": {
        "id": "A6HqMNCHVKRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Postman or curl to send POST requests to http://127.0.0.1:5000/chat"
      ],
      "metadata": {
        "id": "I0_GvQksVNCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 5.2: Deploy the Chatbot\n",
        "Deploy to Heroku (or other platforms):\n",
        "\n",
        "Update the Procfile:\n",
        "\n",
        "Ensure your Procfile includes"
      ],
      "metadata": {
        "id": "lYXUwjhnVPTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "web: python chatbot.py\n"
      ],
      "metadata": {
        "id": "GK_4G76AVQwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploy to Heroku:\n",
        "\n",
        "Push the changes to Heroku or your chosen platform"
      ],
      "metadata": {
        "id": "zHiWGGZqVTfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git add .\n",
        "git commit -m \"Add chatbot\"\n",
        "git push heroku master\n"
      ],
      "metadata": {
        "id": "YjsrqT5VVVDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the Chatbot App:\n",
        "\n",
        "Use the URL provided by Heroku to interact with your chatbot."
      ],
      "metadata": {
        "id": "3E22h3f6VXqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6: Data Quality Enhancement**\n",
        "\\\n",
        "Objective: Implement data cleaning processes and set up data quality monitoring.\n",
        "\n",
        "Sub-step 6.1: Enhance Data Cleaning\n",
        "Improve Data Validation:\n",
        "\n",
        "Update your standardization script to validate important fields before inserting them into the database."
      ],
      "metadata": {
        "id": "zHJPAUQ3VZqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_standardized.dropna(subset=['title', 'latitude', 'longitude'], inplace=True)\n",
        "df_standardized['budget'] = df_standardized['budget'].fillna(0).astype(int)\n"
      ],
      "metadata": {
        "id": "86JPlfWSVjLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Duplicate Entries:\n",
        "\n",
        "Ensure that duplicates are handled before data is inserted"
      ],
      "metadata": {
        "id": "ZpTkYif2VpkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_standardized.drop_duplicates(subset=['original_id'], keep='first', inplace=True)\n"
      ],
      "metadata": {
        "id": "z7sRI8kAVrik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sub-step 6.2: Set Up Data Quality Monitoring\n",
        "Data Quality Scripts:\n",
        "\n",
        "Create scripts that periodically check for missing values, anomalies, or inconsistencies in the database."
      ],
      "metadata": {
        "id": "yZSi88b8VtWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monitor_data_quality():\n",
        "    query = \"SELECT * FROM projects WHERE latitude IS NULL OR longitude IS NULL OR title IS NULL\"\n",
        "    df = pd.read_sql(query, con=engine)\n",
        "    if not df.empty:\n",
        "        print(f\"Data Quality Issues Found: {len(df)} records with missing fields\")\n",
        "    else:\n",
        "        print(\"All records are clean\")\n",
        "\n",
        "monitor_data_quality()\n"
      ],
      "metadata": {
        "id": "uEG_B7U7VxPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ntegrate Alerts:\n",
        "\n",
        "Send email or Slack alerts if any data quality issues are detected"
      ],
      "metadata": {
        "id": "n8_nVm1dVzs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import smtplib\n",
        "\n",
        "def send_alert(message):\n",
        "    server = smtplib.SMTP('smtp.example.com', 587)\n",
        "    server.starttls()\n",
        "    server.login(\"your_email@example.com\", \"password\")\n",
        "    server.sendmail(\"your_email@example.com\", \"recipient@example.com\", message)\n",
        "    server.quit()\n",
        "\n",
        "if not df.empty:\n",
        "    send_alert(f\"Data Quality Issues Detected: {len(df)} records with missing fields\")\n"
      ],
      "metadata": {
        "id": "6ppXgpjhV1Qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Integration and Testing\n",
        "\n",
        "\n",
        "Test All Components Together:\n",
        "\n",
        "Ensure the dashboard, chatbot, and data quality scripts are all functioning as expected.\n",
        "Perform end-to-end tests, simulating real user interactions.\n",
        "Document Everything:\n",
        "\n",
        "Create detailed documentation for future maintenance and updates.\n",
        "Include instructions on how to run, deploy, and monitor each component.\n",
        "Final Deployment:\n",
        "\n",
        "After testing, deploy all components in a production environment, ensuring scalability and reliability."
      ],
      "metadata": {
        "id": "H_8TKNpyV3jT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final Deployment Steps**\n",
        "\n",
        "\n",
        "Step 1: Deploy the Dashboard\n",
        "Finalize and Push the Dashboard to Heroku (or your chosen platform):\n",
        "\n",
        "Ensure the Dashboard is Ready:\n",
        "\n",
        "Verify that all visualizations are working correctly and the app is fully functional locally.\n",
        "Deploy to Heroku:\n"
      ],
      "metadata": {
        "id": "OXUrfgcTV8OT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git add .\n",
        "git commit -m \"Finalize dashboard deployment\"\n",
        "git push heroku master\n"
      ],
      "metadata": {
        "id": "ntpvOkteWBfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Access the Dashboard:\n",
        "\n",
        "After deployment, you can access your dashboard via the Heroku URL provided.\n",
        "Verify Functionality:\n",
        "\n",
        "Test the dashboard on the web to ensure it displays the data correctly and responds well to user interactions.\n",
        "Step 2: Deploy the Chatbot\n",
        "Final Checks for the Chatbot:\n",
        "\n",
        "Ensure the Flask app is running without errors and that the chatbot can interpret queries correctly.\n",
        "Deploy to Heroku:\n",
        "\n",
        "Push the chatbot code to Heroku:"
      ],
      "metadata": {
        "id": "-85WKPjKWEky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git add .\n",
        "git commit -m \"Deploy chatbot\"\n",
        "git push heroku master\n"
      ],
      "metadata": {
        "id": "1ylwLcPMWGDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Chatbot:\n",
        "\n",
        "Use Postman or curl to send POST requests to the chatbot endpoint and verify responses.\n",
        "Ensure it’s connected to the database and returns accurate query results."
      ],
      "metadata": {
        "id": "bCk2_nbqWIXv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Set Up Data Quality Monitoring\n",
        "Deploy Data Quality Monitoring Scripts:\n",
        "\n",
        "Set Up a Cron Job (on Heroku or a Server):\n",
        "\n",
        "If using Heroku, you might use Heroku Scheduler or set up a cron job on a different server to run the data quality scripts periodically.\n"
      ],
      "metadata": {
        "id": "KZtJG4QuWMmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "crontab -e\n"
      ],
      "metadata": {
        "id": "Q43pwUJmWNOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "0 0 * * * /usr/bin/python3 /path/to/monitor_data_quality.py >> /path/to/data_quality.log 2>&1\n"
      ],
      "metadata": {
        "id": "P8nOOh2uWP-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the Monitoring System:\n",
        "\n",
        "Ensure the monitoring script runs as expected and alerts are sent when data quality issues are detected.\n",
        "Verify that alerts are correctly sent via email, Slack, or any other notification system set up.\n",
        "Step 4: Integration and Final Testing\n",
        "Test All Components Together:\n",
        "\n",
        "Dashboard: Ensure it updates automatically based on the data in your database.\n",
        "Chatbot: Verify it responds accurately based on current database entries.\n",
        "Data Quality Monitoring: Ensure that any data inconsistencies trigger alerts and that the database remains clean.\n",
        "Simulate Real User Scenarios:\n",
        "\n",
        "Interact with the dashboard as an end-user would, querying different sectors, budgets, etc.\n",
        "Use the chatbot for natural language queries to verify that it interprets and retrieves data correctly.\n",
        "Introduce deliberate data inconsistencies to test the effectiveness of the monitoring system.\n",
        "Document the Full Setup:\n",
        "\n",
        "Write comprehensive documentation for:\n",
        "Running and maintaining the dashboard.\n",
        "Using and troubleshooting the chatbot.\n",
        "Monitoring and fixing data quality issues.\n",
        "Include details on deployment, configuration files, and any environment-specific settings.\n",
        "Final Deployment and Maintenance Setup:\n",
        "\n",
        "Ensure the entire system is deployed in a production environment, with all components (dashboard, chatbot, monitoring) running smoothly.\n",
        "Set up regular backups of the database and source code repositories."
      ],
      "metadata": {
        "id": "qwWohIL5WTiH"
      }
    }
  ]
}